[2018-08-14 12:04:50,693] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:04:50,740] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:04:50,740] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:04:50,740] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:04:50,740] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2018-08-14 12:04:50,803] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:04:50,804] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2018-08-14 12:04:50,853] INFO Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,853] INFO Server environment:host.name=narendra-LT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,853] INFO Server environment:java.version=1.8.0_171 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,854] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,854] INFO Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,854] INFO Server environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:os.version=4.10.0-28-generic (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:user.name=narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:user.home=/home/narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,855] INFO Server environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,887] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,887] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,888] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:50,960] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:04:58,304] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2018-08-14 12:04:59,230] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:04:59,451] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[2018-08-14 12:04:59,536] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2018-08-14 12:04:59,537] INFO starting (kafka.server.KafkaServer)
[2018-08-14 12:04:59,537] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2018-08-14 12:04:59,633] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:04:59,638] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,638] INFO Client environment:host.name=narendra-LT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,638] INFO Client environment:java.version=1.8.0_171 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,638] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,638] INFO Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,638] INFO Client environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,639] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,639] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,639] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:os.version=4.10.0-28-generic (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:user.name=narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:user.home=/home/narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,640] INFO Client environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,641] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4c12331b (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:04:59,674] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:04:59,674] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:04:59,679] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:04:59,680] INFO Accepted socket connection from /127.0.0.1:49946 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:04:59,689] INFO Client attempting to establish new session at /127.0.0.1:49946 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:59,691] INFO Creating new log file: log.1 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2018-08-14 12:04:59,838] INFO Established session 0x1653724eb020000 with negotiated timeout 6000 for client /127.0.0.1:49946 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:04:59,840] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1653724eb020000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:04:59,843] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:04:59,991] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x2 zxid:0x3 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:00,050] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x6 zxid:0x7 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:00,084] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x9 zxid:0xa txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:00,622] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x15 zxid:0x15 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:00,675] INFO Cluster ID = D76h3nR_S2mriu6Y-ZJNkg (kafka.server.KafkaServer)
[2018-08-14 12:05:00,733] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2018-08-14 12:05:00,912] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:05:00,923] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:05:00,988] INFO [ThrottledRequestReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:05:00,989] INFO [ThrottledRequestReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:05:00,991] INFO [ThrottledRequestReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:05:01,093] INFO Log directory '/tmp/kafka-logs' not found, creating it. (kafka.log.LogManager)
[2018-08-14 12:05:01,123] INFO Loading logs. (kafka.log.LogManager)
[2018-08-14 12:05:01,153] INFO Logs loading complete in 30 ms. (kafka.log.LogManager)
[2018-08-14 12:05:01,165] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2018-08-14 12:05:01,183] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2018-08-14 12:05:01,625] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2018-08-14 12:05:01,677] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2018-08-14 12:05:01,741] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:01,742] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:01,743] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:01,792] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:05:01,891] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:05:01,899] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:05:01,900] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(narendra-LT,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2018-08-14 12:05:01,902] WARN No meta.properties file under dir /tmp/kafka-logs/meta.properties (kafka.server.BrokerMetadataCheckpoint)
[2018-08-14 12:05:02,061] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:02,073] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:05:02,083] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:02,084] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:05:02,111] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:05:02,138] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:05:02,139] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:05:02,141] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:05:02,172] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:setData cxid:0x22 zxid:0x1a txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:02,208] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:0,blockEndProducerId:999) by writing to Zk with path version 1 (kafka.coordinator.transaction.ProducerIdManager)
[2018-08-14 12:05:02,261] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:05:02,264] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:05:02,264] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:05:02,359] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:05:02,374] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:05:02,374] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:05:02,376] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2018-08-14 12:05:02,391] INFO Waiting 10083 ms for the monitored service to finish starting up... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:05:02,407] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:delete cxid:0x37 zxid:0x1d txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:12,498] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:05:12,498] INFO Starting metrics collection from monitored component... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:05:12,921] WARN The replication factor of topic __confluent.support.metrics will be set to 1, which is less than the desired replication factor of 3 (reason: this cluster contains only 1 brokers).  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:05:12,921] INFO Attempting to create topic __confluent.support.metrics with 1 replicas, assuming 1 total brokers (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:05:12,959] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:setData cxid:0x42 zxid:0x1e txntype:-1 reqpath:n/a Error Path:/config/topics/__confluent.support.metrics Error:KeeperErrorCode = NoNode for /config/topics/__confluent.support.metrics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:05:12,999] INFO Topic creation Map(__confluent.support.metrics-0 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2018-08-14 12:05:13,103] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __confluent.support.metrics-0 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:05:13,181] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://narendra-LT:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2018-08-14 12:05:13,219] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:05:13,219] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:05:13,250] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:05:13,306] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 146 ms (kafka.log.Log)
[2018-08-14 12:05:13,312] WARN [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {__confluent.support.metrics=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[2018-08-14 12:05:13,312] INFO Cluster ID: D76h3nR_S2mriu6Y-ZJNkg (org.apache.kafka.clients.Metadata)
[2018-08-14 12:05:13,313] INFO Created log for partition __confluent.support.metrics-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 31536000000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:05:13,314] INFO [Partition __confluent.support.metrics-0 broker=0] No checkpointed highwatermark is found for partition __confluent.support.metrics-0 (kafka.cluster.Partition)
[2018-08-14 12:05:13,318] INFO Replica loaded for partition __confluent.support.metrics-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:05:13,320] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:05:13,350] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:05:13,500] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __confluent.support.metrics-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-08-14 12:05:13,568] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2018-08-14 12:05:13,573] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2018-08-14 12:05:16,519] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2018-08-14 12:06:14,996] INFO Accepted socket connection from /127.0.0.1:49998 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:06:15,002] INFO Client attempting to establish new session at /127.0.0.1:49998 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:15,034] INFO Established session 0x1653724eb020001 with negotiated timeout 30000 for client /127.0.0.1:49998 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:15,692] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020001 type:setData cxid:0x5 zxid:0x25 txntype:-1 reqpath:n/a Error Path:/config/topics/twitter_topic Error:KeeperErrorCode = NoNode for /config/topics/twitter_topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:15,964] INFO Processed session termination for sessionid: 0x1653724eb020001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:16,000] INFO Closed socket connection for client /127.0.0.1:49998 which had sessionid 0x1653724eb020001 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:06:16,087] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions twitter_topic-0 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:06:16,094] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:06:16,095] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:06:16,099] INFO Created log for partition twitter_topic-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:06:16,100] INFO [Partition twitter_topic-0 broker=0] No checkpointed highwatermark is found for partition twitter_topic-0 (kafka.cluster.Partition)
[2018-08-14 12:06:16,100] INFO Replica loaded for partition twitter_topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:06:16,101] INFO [Partition twitter_topic-0 broker=0] twitter_topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:06:16,101] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:06:22,696] INFO Accepted socket connection from /127.0.0.1:50008 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:06:22,700] INFO Client attempting to establish new session at /127.0.0.1:50008 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:22,720] INFO Established session 0x1653724eb020002 with negotiated timeout 30000 for client /127.0.0.1:50008 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:23,212] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020002 type:setData cxid:0x5 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/config/topics/bitcoin_price_topic Error:KeeperErrorCode = NoNode for /config/topics/bitcoin_price_topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:23,361] INFO Processed session termination for sessionid: 0x1653724eb020002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:23,407] INFO Closed socket connection for client /127.0.0.1:50008 which had sessionid 0x1653724eb020002 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:06:23,454] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions bitcoin_price_topic-0 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:06:23,461] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:06:23,461] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:06:23,467] INFO Created log for partition bitcoin_price_topic-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:06:23,566] INFO [Partition bitcoin_price_topic-0 broker=0] No checkpointed highwatermark is found for partition bitcoin_price_topic-0 (kafka.cluster.Partition)
[2018-08-14 12:06:23,567] INFO Replica loaded for partition bitcoin_price_topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:06:23,567] INFO [Partition bitcoin_price_topic-0 broker=0] bitcoin_price_topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:06:23,568] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:06:30,062] INFO Accepted socket connection from /127.0.0.1:50068 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:06:30,066] INFO Client attempting to establish new session at /127.0.0.1:50068 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:30,168] INFO Established session 0x1653724eb020003 with negotiated timeout 30000 for client /127.0.0.1:50068 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:06:30,520] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020003 type:setData cxid:0x5 zxid:0x35 txntype:-1 reqpath:n/a Error Path:/config/topics/bitcoin_transaction_topic Error:KeeperErrorCode = NoNode for /config/topics/bitcoin_transaction_topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:30,815] INFO Processed session termination for sessionid: 0x1653724eb020003 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:06:30,994] INFO Closed socket connection for client /127.0.0.1:50068 which had sessionid 0x1653724eb020003 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:06:31,309] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions bitcoin_transaction_topic-0 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:06:31,315] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:06:31,315] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:06:31,317] INFO Created log for partition bitcoin_transaction_topic-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> [delete], flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 1073741824, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:06:31,318] INFO [Partition bitcoin_transaction_topic-0 broker=0] No checkpointed highwatermark is found for partition bitcoin_transaction_topic-0 (kafka.cluster.Partition)
[2018-08-14 12:06:31,318] INFO Replica loaded for partition bitcoin_transaction_topic-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:06:31,318] INFO [Partition bitcoin_transaction_topic-0 broker=0] bitcoin_transaction_topic-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:06:31,319] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:07:34,927] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:setData cxid:0x70 zxid:0x3c txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:07:34,927] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:setData cxid:0x71 zxid:0x3d txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:07:34,927] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:setData cxid:0x72 zxid:0x3e txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NoNode for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:07:35,135] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x74 zxid:0x40 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:07:35,135] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020000 type:create cxid:0x75 zxid:0x41 txntype:-1 reqpath:n/a Error Path:/config/topics/__consumer_offsets Error:KeeperErrorCode = NodeExists for /config/topics/__consumer_offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:07:35,277] INFO Topic creation Map(__consumer_offsets-22 -> ArrayBuffer(0), __consumer_offsets-30 -> ArrayBuffer(0), __consumer_offsets-8 -> ArrayBuffer(0), __consumer_offsets-21 -> ArrayBuffer(0), __consumer_offsets-4 -> ArrayBuffer(0), __consumer_offsets-27 -> ArrayBuffer(0), __consumer_offsets-7 -> ArrayBuffer(0), __consumer_offsets-9 -> ArrayBuffer(0), __consumer_offsets-46 -> ArrayBuffer(0), __consumer_offsets-25 -> ArrayBuffer(0), __consumer_offsets-35 -> ArrayBuffer(0), __consumer_offsets-41 -> ArrayBuffer(0), __consumer_offsets-33 -> ArrayBuffer(0), __consumer_offsets-23 -> ArrayBuffer(0), __consumer_offsets-49 -> ArrayBuffer(0), __consumer_offsets-47 -> ArrayBuffer(0), __consumer_offsets-16 -> ArrayBuffer(0), __consumer_offsets-28 -> ArrayBuffer(0), __consumer_offsets-31 -> ArrayBuffer(0), __consumer_offsets-36 -> ArrayBuffer(0), __consumer_offsets-42 -> ArrayBuffer(0), __consumer_offsets-3 -> ArrayBuffer(0), __consumer_offsets-18 -> ArrayBuffer(0), __consumer_offsets-37 -> ArrayBuffer(0), __consumer_offsets-15 -> ArrayBuffer(0), __consumer_offsets-24 -> ArrayBuffer(0), __consumer_offsets-38 -> ArrayBuffer(0), __consumer_offsets-17 -> ArrayBuffer(0), __consumer_offsets-48 -> ArrayBuffer(0), __consumer_offsets-19 -> ArrayBuffer(0), __consumer_offsets-11 -> ArrayBuffer(0), __consumer_offsets-13 -> ArrayBuffer(0), __consumer_offsets-2 -> ArrayBuffer(0), __consumer_offsets-43 -> ArrayBuffer(0), __consumer_offsets-6 -> ArrayBuffer(0), __consumer_offsets-14 -> ArrayBuffer(0), __consumer_offsets-20 -> ArrayBuffer(0), __consumer_offsets-0 -> ArrayBuffer(0), __consumer_offsets-44 -> ArrayBuffer(0), __consumer_offsets-39 -> ArrayBuffer(0), __consumer_offsets-12 -> ArrayBuffer(0), __consumer_offsets-45 -> ArrayBuffer(0), __consumer_offsets-1 -> ArrayBuffer(0), __consumer_offsets-5 -> ArrayBuffer(0), __consumer_offsets-26 -> ArrayBuffer(0), __consumer_offsets-29 -> ArrayBuffer(0), __consumer_offsets-34 -> ArrayBuffer(0), __consumer_offsets-10 -> ArrayBuffer(0), __consumer_offsets-32 -> ArrayBuffer(0), __consumer_offsets-40 -> ArrayBuffer(0)) (kafka.zk.AdminZkClient)
[2018-08-14 12:07:35,434] INFO [KafkaApi-0] Auto creation of topic __consumer_offsets with 50 partitions and replication factor 1 is successful (kafka.server.KafkaApis)
[2018-08-14 12:07:38,960] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,__consumer_offsets-38,__consumer_offsets-17,__consumer_offsets-48,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:07:38,970] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:38,971] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:38,972] INFO Created log for partition __consumer_offsets-0 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:38,973] INFO [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
[2018-08-14 12:07:38,973] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:38,973] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:38,989] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:38,991] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2018-08-14 12:07:38,994] INFO Created log for partition __consumer_offsets-29 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:38,995] INFO [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
[2018-08-14 12:07:38,996] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:38,997] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,005] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,006] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,012] INFO Created log for partition __consumer_offsets-48 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,012] INFO [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
[2018-08-14 12:07:39,013] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,013] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,023] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,036] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 15 ms (kafka.log.Log)
[2018-08-14 12:07:39,044] INFO Created log for partition __consumer_offsets-10 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,045] INFO [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
[2018-08-14 12:07:39,045] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,045] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,059] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,060] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2018-08-14 12:07:39,062] INFO Created log for partition __consumer_offsets-45 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,062] INFO [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
[2018-08-14 12:07:39,062] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,063] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,069] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,070] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,071] INFO Created log for partition __consumer_offsets-26 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,072] INFO [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
[2018-08-14 12:07:39,072] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,072] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,076] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,077] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,078] INFO Created log for partition __consumer_offsets-7 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,078] INFO [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
[2018-08-14 12:07:39,079] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,079] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,085] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,086] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:07:39,087] INFO Created log for partition __consumer_offsets-42 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,089] INFO [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
[2018-08-14 12:07:39,089] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,090] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,100] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,103] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2018-08-14 12:07:39,104] INFO Created log for partition __consumer_offsets-4 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,105] INFO [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
[2018-08-14 12:07:39,105] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,105] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,108] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,109] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,110] INFO Created log for partition __consumer_offsets-23 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,111] INFO [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
[2018-08-14 12:07:39,111] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,111] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,118] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,119] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,121] INFO Created log for partition __consumer_offsets-1 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,121] INFO [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,121] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,121] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,126] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,126] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,127] INFO Created log for partition __consumer_offsets-20 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,128] INFO [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
[2018-08-14 12:07:39,128] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,128] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,140] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,141] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,142] INFO Created log for partition __consumer_offsets-39 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,143] INFO [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
[2018-08-14 12:07:39,143] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,143] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,155] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,173] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 27 ms (kafka.log.Log)
[2018-08-14 12:07:39,174] INFO Created log for partition __consumer_offsets-17 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,175] INFO [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
[2018-08-14 12:07:39,175] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,175] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,178] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,179] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,189] INFO Created log for partition __consumer_offsets-36 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,190] INFO [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
[2018-08-14 12:07:39,190] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,190] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,202] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,202] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,204] INFO Created log for partition __consumer_offsets-14 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,204] INFO [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
[2018-08-14 12:07:39,205] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,205] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,211] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,212] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,213] INFO Created log for partition __consumer_offsets-33 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,214] INFO [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
[2018-08-14 12:07:39,217] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,217] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,237] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,237] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,239] INFO Created log for partition __consumer_offsets-49 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,239] INFO [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
[2018-08-14 12:07:39,239] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,239] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,255] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,261] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 7 ms (kafka.log.Log)
[2018-08-14 12:07:39,263] INFO Created log for partition __consumer_offsets-11 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,272] INFO [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
[2018-08-14 12:07:39,272] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,272] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,276] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,277] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,278] INFO Created log for partition __consumer_offsets-30 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,278] INFO [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
[2018-08-14 12:07:39,279] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,279] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,338] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,339] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:39,340] INFO Created log for partition __consumer_offsets-46 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,341] INFO [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
[2018-08-14 12:07:39,341] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,341] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,346] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,346] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,351] INFO Created log for partition __consumer_offsets-27 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,352] INFO [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
[2018-08-14 12:07:39,352] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,352] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,359] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,359] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,402] INFO Created log for partition __consumer_offsets-8 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,402] INFO [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
[2018-08-14 12:07:39,403] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,403] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,416] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,419] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2018-08-14 12:07:39,421] INFO Created log for partition __consumer_offsets-24 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,421] INFO [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
[2018-08-14 12:07:39,421] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,421] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,426] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,426] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,429] INFO Created log for partition __consumer_offsets-43 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,429] INFO [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
[2018-08-14 12:07:39,431] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,431] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,440] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,492] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 54 ms (kafka.log.Log)
[2018-08-14 12:07:39,502] INFO Created log for partition __consumer_offsets-5 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,503] INFO [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
[2018-08-14 12:07:39,503] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,503] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,506] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,507] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,508] INFO Created log for partition __consumer_offsets-21 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,509] INFO [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
[2018-08-14 12:07:39,509] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,509] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,514] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,515] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:39,519] INFO Created log for partition __consumer_offsets-2 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,519] INFO [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
[2018-08-14 12:07:39,519] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,520] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,523] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,524] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,525] INFO Created log for partition __consumer_offsets-40 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,525] INFO [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
[2018-08-14 12:07:39,525] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,526] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,529] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,531] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:39,532] INFO Created log for partition __consumer_offsets-37 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,533] INFO [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
[2018-08-14 12:07:39,533] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,533] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,537] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,538] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,539] INFO Created log for partition __consumer_offsets-18 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,539] INFO [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
[2018-08-14 12:07:39,539] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,539] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,542] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,543] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,543] INFO Created log for partition __consumer_offsets-34 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,544] INFO [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
[2018-08-14 12:07:39,544] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,544] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,548] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,548] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,550] INFO Created log for partition __consumer_offsets-15 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,550] INFO [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
[2018-08-14 12:07:39,550] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,551] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,554] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,555] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,556] INFO Created log for partition __consumer_offsets-12 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,556] INFO [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
[2018-08-14 12:07:39,556] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,556] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,559] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,560] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,561] INFO Created log for partition __consumer_offsets-31 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,561] INFO [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
[2018-08-14 12:07:39,562] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,562] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,566] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,566] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,567] INFO Created log for partition __consumer_offsets-9 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,568] INFO [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
[2018-08-14 12:07:39,568] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,568] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,572] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,572] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,573] INFO Created log for partition __consumer_offsets-47 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,574] INFO [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
[2018-08-14 12:07:39,574] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,574] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,577] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,578] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,579] INFO Created log for partition __consumer_offsets-19 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,579] INFO [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
[2018-08-14 12:07:39,580] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,580] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,587] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,588] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,589] INFO Created log for partition __consumer_offsets-28 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,590] INFO [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
[2018-08-14 12:07:39,590] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,590] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,593] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,593] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,594] INFO Created log for partition __consumer_offsets-38 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,594] INFO [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
[2018-08-14 12:07:39,594] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,595] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,601] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,602] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:39,605] INFO Created log for partition __consumer_offsets-35 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,605] INFO [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
[2018-08-14 12:07:39,605] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,605] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,610] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,610] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,618] INFO Created log for partition __consumer_offsets-44 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,619] INFO [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
[2018-08-14 12:07:39,619] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,619] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,623] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,624] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,625] INFO Created log for partition __consumer_offsets-6 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,626] INFO [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
[2018-08-14 12:07:39,626] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,626] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,639] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,640] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2018-08-14 12:07:39,641] INFO Created log for partition __consumer_offsets-25 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,641] INFO [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
[2018-08-14 12:07:39,641] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,641] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,645] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,645] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,646] INFO Created log for partition __consumer_offsets-16 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,647] INFO [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
[2018-08-14 12:07:39,648] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,648] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,653] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,653] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,654] INFO Created log for partition __consumer_offsets-22 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,655] INFO [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
[2018-08-14 12:07:39,655] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,655] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,658] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,659] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:07:39,660] INFO Created log for partition __consumer_offsets-41 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,661] INFO [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
[2018-08-14 12:07:39,661] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,661] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,666] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,667] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:07:39,669] INFO Created log for partition __consumer_offsets-32 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,669] INFO [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
[2018-08-14 12:07:39,669] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,669] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,673] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,673] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:07:39,675] INFO Created log for partition __consumer_offsets-3 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,676] INFO [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
[2018-08-14 12:07:39,676] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,676] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,682] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:07:39,683] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:07:39,685] INFO Created log for partition __consumer_offsets-13 in /tmp/kafka-logs with properties {compression.type -> producer, message.format.version -> 1.1-IV0, file.delete.delay.ms -> 60000, max.message.bytes -> 1000012, min.compaction.lag.ms -> 0, message.timestamp.type -> CreateTime, min.insync.replicas -> 1, segment.jitter.ms -> 0, preallocate -> false, min.cleanable.dirty.ratio -> 0.5, index.interval.bytes -> 4096, unclean.leader.election.enable -> false, retention.bytes -> -1, delete.retention.ms -> 86400000, cleanup.policy -> compact, flush.ms -> 9223372036854775807, segment.ms -> 604800000, segment.bytes -> 104857600, retention.ms -> 604800000, message.timestamp.difference.max.ms -> 9223372036854775807, segment.index.bytes -> 10485760, flush.messages -> 9223372036854775807}. (kafka.log.LogManager)
[2018-08-14 12:07:39,686] INFO [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
[2018-08-14 12:07:39,686] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:07:39,686] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:07:39,689] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:07:41,038] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,228] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,229] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,230] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,230] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,230] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,231] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,231] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,233] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,233] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,233] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,234] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,243] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 14 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,250] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,256] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 5 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,256] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,256] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,257] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,258] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,259] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,260] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,261] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,272] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,273] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,274] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,275] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,275] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,275] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,275] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,275] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:07:42,390] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 0 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:07:42,442] INFO [GroupCoordinator 0]: Stabilized group flume generation 1 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:07:42,467] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 1 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:07:42,479] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: __consumer_offsets-37. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-08-14 12:07:48,824] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: bitcoin_transaction_topic-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-08-14 12:08:27,272] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: twitter_topic-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-08-14 12:09:47,819] INFO Updated PartitionLeaderEpoch. New: {epoch:0, offset:0}, Current: {epoch:-1, offset:-1} for Partition: bitcoin_price_topic-0. Cache now contains 0 entries. (kafka.server.epoch.LeaderEpochFileCache)
[2018-08-14 12:12:42,842] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 1 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:12:47,057] INFO [GroupCoordinator 0]: Stabilized group flume generation 2 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:12:47,059] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 2 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:14:25,473] INFO Accepted socket connection from /127.0.0.1:56232 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:14:25,477] INFO Client attempting to establish new session at /127.0.0.1:56232 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:25,496] INFO Established session 0x1653724eb020004 with negotiated timeout 30000 for client /127.0.0.1:56232 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:25,565] INFO Processed session termination for sessionid: 0x1653724eb020004 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:14:25,586] INFO Closed socket connection for client /127.0.0.1:56232 which had sessionid 0x1653724eb020004 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:14:51,214] INFO Accepted socket connection from /127.0.0.1:56618 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:14:51,220] INFO Client attempting to establish new session at /127.0.0.1:56618 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:51,233] INFO Established session 0x1653724eb020005 with negotiated timeout 30000 for client /127.0.0.1:56618 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:51,395] INFO Accepted socket connection from /127.0.0.1:56622 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:14:51,395] INFO Client attempting to establish new session at /127.0.0.1:56622 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:51,411] INFO Established session 0x1653724eb020006 with negotiated timeout 6000 for client /127.0.0.1:56622 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:14:51,717] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020006 type:create cxid:0x2 zxid:0xac txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:14:51,927] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020006 type:create cxid:0x19 zxid:0xb0 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-24237/owners/twitter_topic Error:KeeperErrorCode = NoNode for /consumers/console-consumer-24237/owners/twitter_topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:14:51,942] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020006 type:create cxid:0x1a zxid:0xb1 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-24237/owners Error:KeeperErrorCode = NoNode for /consumers/console-consumer-24237/owners (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:15:02,142] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 3 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:15:51,448] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020006 type:setData cxid:0x24 zxid:0xb5 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-24237/offsets/twitter_topic/0 Error:KeeperErrorCode = NoNode for /consumers/console-consumer-24237/offsets/twitter_topic/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:15:51,467] INFO Got user-level KeeperException when processing sessionid:0x1653724eb020006 type:create cxid:0x25 zxid:0xb6 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-24237/offsets Error:KeeperErrorCode = NoNode for /consumers/console-consumer-24237/offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:16:40,105] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 2 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:16:41,199] INFO [GroupCoordinator 0]: Stabilized group flume generation 3 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:16:41,201] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 3 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:17:38,712] INFO Terminating process due to signal SIGTERM (io.confluent.support.metrics.SupportedKafka)
[2018-08-14 12:17:38,738] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:17:38,744] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://narendra-LT:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2018-08-14 12:17:38,766] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:17:38,766] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:17:38,778] INFO Cluster ID: D76h3nR_S2mriu6Y-ZJNkg (org.apache.kafka.clients.Metadata)
[2018-08-14 12:17:38,797] INFO [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2018-08-14 12:17:38,800] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2018-08-14 12:17:40,049] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2018-08-14 12:17:40,049] INFO Graceful terminating metrics collection because the monitored component is shutting down... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:17:40,049] INFO Metrics collection stopped (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:17:40,049] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2018-08-14 12:17:40,050] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2018-08-14 12:17:40,103] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2018-08-14 12:17:40,108] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:17:40,109] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:17:40,109] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:17:40,109] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2018-08-14 12:17:40,117] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2018-08-14 12:17:40,118] INFO [Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2018-08-14 12:17:40,132] INFO [Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2018-08-14 12:17:40,150] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2018-08-14 12:17:40,152] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,189] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,189] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,197] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:17:40,199] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 0 (kafka.coordinator.transaction.ProducerIdManager)
[2018-08-14 12:17:40,200] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2018-08-14 12:17:40,200] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:17:40,200] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:17:40,200] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:17:40,201] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:17:40,214] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:17:40,214] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,289] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,289] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,290] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,312] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,312] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,313] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:17:40,315] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2018-08-14 12:17:40,315] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:17:40,315] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:17:40,315] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:17:40,316] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:17:40,318] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:17:40,318] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:17:40,319] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:17:40,319] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,361] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,361] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,362] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,380] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,380] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,381] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,581] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,581] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:17:40,671] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2018-08-14 12:17:40,673] INFO Shutting down. (kafka.log.LogManager)
[2018-08-14 12:17:40,763] INFO [ProducerStateManager partition=__consumer_offsets-37] Writing producer snapshot at offset 72 (kafka.log.ProducerStateManager)
[2018-08-14 12:17:40,765] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Writing producer snapshot at offset 485 (kafka.log.ProducerStateManager)
[2018-08-14 12:17:40,766] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Writing producer snapshot at offset 2293 (kafka.log.ProducerStateManager)
[2018-08-14 12:17:40,860] INFO [ProducerStateManager partition=twitter_topic-0] Writing producer snapshot at offset 688 (kafka.log.ProducerStateManager)
[2018-08-14 12:17:40,878] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Writing producer snapshot at offset 2 (kafka.log.ProducerStateManager)
[2018-08-14 12:17:40,988] INFO Shutdown complete. (kafka.log.LogManager)
[2018-08-14 12:17:41,019] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:17:41,020] INFO Processed session termination for sessionid: 0x1653724eb020000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:17:41,037] INFO Session: 0x1653724eb020000 closed (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:17:41,038] INFO EventThread shut down for session: 0x1653724eb020000 (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:17:41,039] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:17:41,038] INFO Closed socket connection for client /127.0.0.1:49946 which had sessionid 0x1653724eb020000 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:17:41,039] INFO [ThrottledRequestReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,179] INFO [ThrottledRequestReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,179] INFO [ThrottledRequestReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,180] INFO [ThrottledRequestReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,184] INFO [ThrottledRequestReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,184] INFO [ThrottledRequestReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,184] INFO [ThrottledRequestReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,230] INFO [ThrottledRequestReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,231] INFO [ThrottledRequestReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:17:41,232] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2018-08-14 12:17:41,299] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2018-08-14 12:17:41,314] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
[2018-08-14 12:22:38,044] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:22:38,047] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:22:38,047] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:22:38,047] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:22:38,048] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2018-08-14 12:22:38,076] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:22:38,076] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2018-08-14 12:22:38,094] INFO Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,095] INFO Server environment:host.name=narendra-LT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,095] INFO Server environment:java.version=1.8.0_171 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,095] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,095] INFO Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,095] INFO Server environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,096] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,096] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,096] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,096] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,097] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,097] INFO Server environment:os.version=4.10.0-28-generic (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,097] INFO Server environment:user.name=narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,097] INFO Server environment:user.home=/home/narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,097] INFO Server environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,104] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,104] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,104] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:38,117] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:22:45,000] INFO Expiring session 0x1653724eb020006, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:22:45,002] INFO Processed session termination for sessionid: 0x1653724eb020006 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:22:45,003] INFO Creating new log file: log.bd (org.apache.zookeeper.server.persistence.FileTxnLog)
[2018-08-14 12:23:03,131] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2018-08-14 12:23:03,470] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:23:03,555] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[2018-08-14 12:23:03,596] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2018-08-14 12:23:03,598] INFO starting (kafka.server.KafkaServer)
[2018-08-14 12:23:03,599] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2018-08-14 12:23:03,615] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:23:03,625] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,625] INFO Client environment:host.name=narendra-LT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,625] INFO Client environment:java.version=1.8.0_171 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,625] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,625] INFO Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,625] INFO Client environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:os.version=4.10.0-28-generic (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,627] INFO Client environment:user.name=narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,628] INFO Client environment:user.home=/home/narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,628] INFO Client environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,629] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4c12331b (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:23:03,642] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:23:03,643] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:23:03,648] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:23:03,649] INFO Accepted socket connection from /127.0.0.1:59994 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:23:03,667] INFO Client attempting to establish new session at /127.0.0.1:59994 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:23:03,688] INFO Established session 0x165373533b30000 with negotiated timeout 6000 for client /127.0.0.1:59994 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:23:03,691] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x165373533b30000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:23:03,695] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:23:03,731] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x1 zxid:0xbf txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,751] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x2 zxid:0xc0 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,765] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x3 zxid:0xc1 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,776] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x4 zxid:0xc2 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,787] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x5 zxid:0xc3 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,798] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x6 zxid:0xc4 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,810] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x7 zxid:0xc5 txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,820] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x8 zxid:0xc6 txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,832] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0x9 zxid:0xc7 txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,842] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0xa zxid:0xc8 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,852] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0xb zxid:0xc9 txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,864] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0xc zxid:0xca txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:03,874] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:create cxid:0xd zxid:0xcb txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:04,105] INFO Cluster ID = D76h3nR_S2mriu6Y-ZJNkg (kafka.server.KafkaServer)
[2018-08-14 12:23:04,206] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:23:04,225] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:23:04,272] INFO [ThrottledRequestReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:23:04,273] INFO [ThrottledRequestReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:23:04,274] INFO [ThrottledRequestReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:23:04,337] INFO Loading logs. (kafka.log.LogManager)
[2018-08-14 12:23:04,402] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,411] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 44 ms (kafka.log.Log)
[2018-08-14 12:23:04,424] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,424] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,429] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,430] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,438] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,439] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,446] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,446] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,453] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,453] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,458] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,459] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,464] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,465] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,473] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,473] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,477] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,477] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,482] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,482] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,488] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,488] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,493] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,493] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,498] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,498] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,504] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,505] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:23:04,510] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,510] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,514] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,514] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,518] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,519] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,523] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,523] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,527] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,527] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,542] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 2293 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,558] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/bitcoin_transaction_topic-0/00000000000000002293.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:23:04,594] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2293 in 63 ms (kafka.log.Log)
[2018-08-14 12:23:04,601] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,602] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,607] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,608] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,614] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,615] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,618] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,619] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,622] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,623] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,626] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,627] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,632] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 688 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,633] INFO [ProducerStateManager partition=twitter_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/twitter_topic-0/00000000000000000688.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:23:04,633] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 688 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,638] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,639] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,644] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,645] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,652] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,652] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,657] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state from offset 72 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,657] INFO [ProducerStateManager partition=__consumer_offsets-37] Loading producer state from snapshot file '/tmp/kafka-logs/__consumer_offsets-37/00000000000000000072.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:23:04,658] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 72 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,661] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,662] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,665] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,666] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,686] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,686] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,691] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,692] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,709] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state from offset 2 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,709] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Loading producer state from snapshot file '/tmp/kafka-logs/__confluent.support.metrics-0/00000000000000000002.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:23:04,710] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 2 in 3 ms (kafka.log.Log)
[2018-08-14 12:23:04,714] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,717] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:23:04,722] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,722] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 1 ms (kafka.log.Log)
[2018-08-14 12:23:04,725] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,726] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,729] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 485 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,730] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/bitcoin_price_topic-0/00000000000000000485.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:23:04,730] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 485 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,745] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,749] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 17 ms (kafka.log.Log)
[2018-08-14 12:23:04,762] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,762] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,768] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,768] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,772] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,772] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,776] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,777] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,779] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,780] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,784] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,784] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,791] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,791] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,795] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,795] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,799] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,799] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,802] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,803] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,806] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,807] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,811] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:23:04,811] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:23:04,814] INFO Logs loading complete in 477 ms. (kafka.log.LogManager)
[2018-08-14 12:23:04,839] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2018-08-14 12:23:04,840] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2018-08-14 12:23:05,116] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2018-08-14 12:23:05,188] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2018-08-14 12:23:05,312] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,312] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,313] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,340] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:23:05,417] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:23:05,436] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:23:05,438] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(narendra-LT,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2018-08-14 12:23:05,505] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,506] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,506] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:23:05,508] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:23:05,527] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:23:05,549] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:05,550] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:05,551] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:05,591] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:1000,blockEndProducerId:1999) by writing to Zk with path version 2 (kafka.coordinator.transaction.ProducerIdManager)
[2018-08-14 12:23:05,622] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:23:05,623] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:23:05,623] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:23:05,693] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:23:05,717] WARN [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] retention.ms for topic __confluent.support.metrics is set to 31536000000. It is smaller than message.timestamp.difference.max.ms's value 9223372036854775807. This may result in frequent log rolling. (kafka.log.Log)
[2018-08-14 12:23:05,773] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:23:05,773] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:23:05,780] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2018-08-14 12:23:05,786] INFO Waiting 10093 ms for the monitored service to finish starting up... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:23:05,907] INFO Got user-level KeeperException when processing sessionid:0x165373533b30000 type:delete cxid:0x72 zxid:0xd0 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:05,968] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,bitcoin_price_topic-0,bitcoin_transaction_topic-0,__consumer_offsets-38,__consumer_offsets-17,twitter_topic-0,__consumer_offsets-48,__confluent.support.metrics-0,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:23:05,995] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:05,997] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,014] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,014] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,019] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,020] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,025] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,025] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,029] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,029] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,033] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,033] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,038] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,038] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,042] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,043] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,047] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,047] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,051] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,051] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,055] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,055] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,058] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,059] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,062] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,062] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,066] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,066] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,070] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,071] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,074] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,075] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,078] INFO Replica loaded for partition bitcoin_price_topic-0 with initial high watermark 485 (kafka.cluster.Replica)
[2018-08-14 12:23:06,079] INFO [Partition bitcoin_price_topic-0 broker=0] bitcoin_price_topic-0 starts at Leader Epoch 0 from offset 485. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,083] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,083] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,087] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,087] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,090] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,090] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,093] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,093] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,096] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,097] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,100] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,100] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,103] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,103] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,106] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,106] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,109] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,109] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,113] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,113] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,116] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,117] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,120] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,120] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,123] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,123] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,125] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 72 (kafka.cluster.Replica)
[2018-08-14 12:23:06,125] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 72. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,128] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,129] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,138] INFO Replica loaded for partition __confluent.support.metrics-0 with initial high watermark 2 (kafka.cluster.Replica)
[2018-08-14 12:23:06,138] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 2. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,141] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,142] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,145] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,146] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,149] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,150] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,155] INFO Replica loaded for partition twitter_topic-0 with initial high watermark 688 (kafka.cluster.Replica)
[2018-08-14 12:23:06,155] INFO [Partition twitter_topic-0 broker=0] twitter_topic-0 starts at Leader Epoch 0 from offset 688. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,158] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,158] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,161] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,161] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,164] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,165] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,171] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,171] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,174] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,174] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,177] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,178] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,181] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,181] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,184] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,184] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,189] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,189] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,191] INFO Replica loaded for partition bitcoin_transaction_topic-0 with initial high watermark 2293 (kafka.cluster.Replica)
[2018-08-14 12:23:06,191] INFO [Partition bitcoin_transaction_topic-0 broker=0] bitcoin_transaction_topic-0 starts at Leader Epoch 0 from offset 2293. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,194] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,194] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,201] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,201] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,206] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,206] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,209] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,209] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,212] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,213] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,216] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,217] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,222] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:23:06,223] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:23:06,240] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:23:06,242] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,242] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,243] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,244] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,245] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,247] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,247] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,247] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,248] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,249] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,249] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,268] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 24 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,269] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,270] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,323] INFO [GroupCoordinator 0]: Loading group metadata for flume with generation 3 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:06,341] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 71 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,341] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,342] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,342] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,343] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,343] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,343] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,344] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,344] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,345] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,345] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,345] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,346] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,346] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,346] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,347] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,347] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,348] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,348] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,348] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,349] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,349] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,349] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,350] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,350] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,351] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,351] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,352] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,352] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,352] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,353] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,353] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,353] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,353] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,354] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,354] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,354] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,354] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,355] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,355] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,355] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,356] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,356] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,357] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,357] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:06,357] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:23:09,000] INFO Expiring session 0x1653724eb020005, timeout of 30000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:23:09,000] INFO Processed session termination for sessionid: 0x1653724eb020005 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:23:15,888] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:23:15,888] INFO Starting metrics collection from monitored component... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:23:16,207] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:23:16,224] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://narendra-LT:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2018-08-14 12:23:16,286] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:23:16,287] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:23:16,321] INFO Cluster ID: D76h3nR_S2mriu6Y-ZJNkg (org.apache.kafka.clients.Metadata)
[2018-08-14 12:23:16,413] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2018-08-14 12:23:16,422] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2018-08-14 12:23:18,238] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2018-08-14 12:23:36,343] INFO [GroupCoordinator 0]: Member consumer-2-be0c22d7-e36a-42fd-83f7-6f1d44daca84 in group flume has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:36,347] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 3 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:36,351] INFO [GroupCoordinator 0]: Member consumer-1-2aed2786-e17b-45ee-bbf1-f21b34a3452d in group flume has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:36,352] INFO [GroupCoordinator 0]: Member consumer-3-28de4882-b92a-4b5f-aa96-e33574135d82 in group flume has failed, removing it from the group (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:23:36,355] INFO [GroupCoordinator 0]: Group flume with generation 4 is now empty (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:29:25,655] INFO Unable to read additional data from server sessionid 0x165373533b30000, likely server has closed socket, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:27,504] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:27,506] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:29,219] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:29,219] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:30,412] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:30,412] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:31,485] INFO Terminating process due to signal SIGHUP (io.confluent.support.metrics.SupportedKafka)
[2018-08-14 12:29:31,485] INFO Terminating process due to signal SIGHUP (io.confluent.support.metrics.SupportedKafka)
[2018-08-14 12:29:31,702] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:31,703] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:31,807] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:29:33,342] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:33,343] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:34,880] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:34,880] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:36,287] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:36,288] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:37,400] INFO Terminating process due to signal SIGTERM (io.confluent.support.metrics.SupportedKafka)
[2018-08-14 12:29:38,077] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:38,078] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:40,033] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:40,033] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:41,291] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:41,291] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:42,724] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:42,725] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:43,941] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:43,941] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:45,446] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:45,446] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:47,480] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:47,481] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:49,576] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:49,576] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:51,205] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:51,206] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:52,562] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:52,563] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:54,416] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:54,417] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:56,137] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:56,137] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:57,720] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:57,721] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:29:59,553] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:29:59,554] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:01,406] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:01,406] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:03,288] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:03,288] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:05,322] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:05,322] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:06,703] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:06,703] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:08,111] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:08,112] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:09,452] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:09,453] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:10,681] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:10,682] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:11,869] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:11,869] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:13,052] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:13,052] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:14,329] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:14,329] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:16,032] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:16,033] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:17,733] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:17,734] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:19,159] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:19,160] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:21,125] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:21,126] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:23,225] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:23,226] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:24,687] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:24,687] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:26,771] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:26,771] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:28,453] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:28,454] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:30,168] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:30,168] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:31,719] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:31,719] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:33,544] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:33,544] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:34,662] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:34,663] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:35,786] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:35,787] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:37,036] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:37,037] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:38,448] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:38,449] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:39,596] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:39,596] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:40,998] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:40,998] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:43,047] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:43,048] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:44,957] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:44,958] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:46,877] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:46,878] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:48,881] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:48,881] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:50,247] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:50,247] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:51,349] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:51,350] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:53,090] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:53,090] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:54,202] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:54,203] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:56,163] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:56,163] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:57,340] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:57,340] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:30:58,677] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:30:58,678] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:00,532] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:00,532] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:02,208] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:02,208] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:04,208] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:04,208] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:06,140] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:06,141] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:08,068] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:08,069] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:10,093] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:10,093] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:11,853] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:11,853] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:13,134] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:13,134] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:14,989] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:14,989] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:17,010] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:17,011] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:18,908] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:18,909] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:20,312] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:20,313] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:22,095] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:22,096] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:23,377] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:23,377] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:24,520] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:24,520] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:25,730] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:25,730] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:27,081] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:27,081] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:29,173] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:29,173] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:30,629] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:30,630] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:31,803] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:31,804] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:33,512] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:33,512] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:35,547] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:35,547] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:36,987] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:36,987] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:38,770] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:38,770] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:40,372] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:40,372] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:42,428] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:42,429] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:44,154] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:44,155] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:46,184] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:46,185] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:48,234] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:48,234] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:50,133] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:50,133] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:51,607] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:51,608] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:53,438] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:53,438] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:55,469] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:55,469] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:56,950] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:56,950] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:31:58,753] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:31:58,753] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:32:00,101] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:32:00,102] WARN Session 0x165373533b30000 for server null, unexpected error, closing socket connection and attempting reconnect (org.apache.zookeeper.ClientCnxn)
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141)
[2018-08-14 12:34:38,335] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:34:38,339] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:34:38,339] INFO autopurge.purgeInterval set to 0 (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:34:38,339] INFO Purge task is not scheduled. (org.apache.zookeeper.server.DatadirCleanupManager)
[2018-08-14 12:34:38,339] WARN Either no config or no quorum defined in config, running  in standalone mode (org.apache.zookeeper.server.quorum.QuorumPeerMain)
[2018-08-14 12:34:38,366] INFO Reading configuration from: ./confluent-4.1.0/etc/kafka/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)
[2018-08-14 12:34:38,366] INFO Starting server (org.apache.zookeeper.server.ZooKeeperServerMain)
[2018-08-14 12:34:38,374] INFO Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,375] INFO Server environment:host.name=narendra-LT (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,375] INFO Server environment:java.version=1.8.0_171 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,375] INFO Server environment:java.vendor=Oracle Corporation (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,375] INFO Server environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,375] INFO Server environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:java.io.tmpdir=/tmp (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:java.compiler=<NA> (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:os.name=Linux (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:os.arch=amd64 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:os.version=4.10.0-28-generic (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,376] INFO Server environment:user.name=narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,377] INFO Server environment:user.home=/home/narendra (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,377] INFO Server environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,383] INFO tickTime set to 3000 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,383] INFO minSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,383] INFO maxSessionTimeout set to -1 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:38,392] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:34:45,000] INFO Expiring session 0x165373533b30000, timeout of 6000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:45,002] INFO Processed session termination for sessionid: 0x165373533b30000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:45,003] INFO Creating new log file: log.d2 (org.apache.zookeeper.server.persistence.FileTxnLog)
[2018-08-14 12:34:48,452] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2018-08-14 12:34:48,767] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:34:48,827] WARN The package io.confluent.support.metrics.collectors.FullCollector for collecting the full set of support metrics could not be loaded, so we are reverting to anonymous, basic metric collection. If you are a Confluent customer, please refer to the Confluent Platform documentation, section Proactive Support, on how to activate full metrics collection. (io.confluent.support.metrics.KafkaSupportConfig)
[2018-08-14 12:34:48,851] WARN Please note that the support metrics collection feature ("Metrics") of Proactive Support is enabled.  With Metrics enabled, this broker is configured to collect and report certain broker and cluster metadata ("Metadata") about your use of the Confluent Platform (including without limitation, your remote internet protocol address) to Confluent, Inc. ("Confluent") or its parent, subsidiaries, affiliates or service providers every 24hours.  This Metadata may be transferred to any country in which Confluent maintains facilities.  For a more in depth discussion of how Confluent processes such information, please read our Privacy Policy located at http://www.confluent.io/privacy. By proceeding with `confluent.support.metrics.enable=true`, you agree to all such collection, transfer, storage and use of Metadata by Confluent.  You can turn the Metrics feature off by setting `confluent.support.metrics.enable=false` in the broker configuration and restarting the broker.  See the Confluent Platform documentation for further information. (io.confluent.support.metrics.SupportedServerStartable)
[2018-08-14 12:34:48,852] INFO starting (kafka.server.KafkaServer)
[2018-08-14 12:34:48,853] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)
[2018-08-14 12:34:48,870] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:34:48,877] INFO Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,877] INFO Client environment:host.name=narendra-LT (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,877] INFO Client environment:java.version=1.8.0_171 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,878] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,878] INFO Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,878] INFO Client environment:java.class.path=/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-client-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-tools-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.ws.rs-api-2.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-beanutils-1.8.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-api-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-client-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zookeeper-3.4.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/aopalliance-repackaged-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-utils-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.20.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-media-jaxb-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/osgi-resource-locator-1.0.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-security-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/log4j-1.2.17.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-base-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-compress-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/common-utils-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-api-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-server-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-clients-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-log4j-appender-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/lz4-java-1.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-digester-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-test-utils-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/guava-20.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/rocksdbjni-5.7.3.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-scaladoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/argparse4j-0.7.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/plexus-utils-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-http-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-runtime-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-util-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/xz-1.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.annotation-api-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-reflect-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/zkclient-0.10.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpcore-4.4.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-annotations-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlets-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-guava-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-server-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-common-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-javadoc.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jline-0.9.94.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-validator-1.4.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-io-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-json-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/netty-3.10.5.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpclient-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/snappy-java-1.1.7.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-transforms-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/avro-1.8.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-core-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-collections-3.2.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-common-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/maven-artifact-3.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javassist-3.21.0-GA.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-logging-1.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-continuation-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jetty-servlet-9.2.24.v20180105.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/metrics-core-2.2.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-logging_2.11-3.7.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-lang3-3.5.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/hk2-locator-2.5.0-b32.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jersey-container-servlet-core-2.25.1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-mapper-asl-1.9.13.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/commons-codec-1.9.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/reflections-0.9.11.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/paranamer-2.7.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/scala-library-2.11.12.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.inject-1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-api-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka-streams-examples-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/support-metrics-client-4.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-databind-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/validation-api-1.1.0.Final.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/slf4j-log4j12-1.7.25.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-sources.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/httpmime-4.5.2.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.9.4.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/connect-file-1.1.0-cp1.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/kafka/kafka_2.11-1.1.0-cp1-test.jar:/home/narendra/Desktop/Datalake/data_lake_pipeline/confluent-4.1.0/bin/../share/java/confluent-support-metrics/*:/usr/share/java/confluent-support-metrics/* (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:java.io.tmpdir=/tmp (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:java.compiler=<NA> (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:os.name=Linux (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:os.arch=amd64 (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:os.version=4.10.0-28-generic (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:user.name=narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:user.home=/home/narendra (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,879] INFO Client environment:user.dir=/home/narendra/Desktop/Datalake/data_lake_pipeline (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,881] INFO Initiating client connection, connectString=localhost:2181 sessionTimeout=6000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4c12331b (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:34:48,893] INFO [ZooKeeperClient] Waiting until connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:34:48,894] INFO Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error) (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:34:48,899] INFO Socket connection established to localhost/127.0.0.1:2181, initiating session (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:34:48,900] INFO Accepted socket connection from /127.0.0.1:33216 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:34:48,908] INFO Client attempting to establish new session at /127.0.0.1:33216 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:48,923] INFO Established session 0x1653740312d0000 with negotiated timeout 6000 for client /127.0.0.1:33216 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:34:48,926] INFO Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1653740312d0000, negotiated timeout = 6000 (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:34:48,929] INFO [ZooKeeperClient] Connected. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:34:48,963] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x1 zxid:0xd4 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:48,985] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x2 zxid:0xd5 txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:48,999] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x3 zxid:0xd6 txntype:-1 reqpath:n/a Error Path:/brokers/topics Error:KeeperErrorCode = NodeExists for /brokers/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,010] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x4 zxid:0xd7 txntype:-1 reqpath:n/a Error Path:/config/changes Error:KeeperErrorCode = NodeExists for /config/changes (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,021] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x5 zxid:0xd8 txntype:-1 reqpath:n/a Error Path:/admin/delete_topics Error:KeeperErrorCode = NodeExists for /admin/delete_topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,033] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x6 zxid:0xd9 txntype:-1 reqpath:n/a Error Path:/brokers/seqid Error:KeeperErrorCode = NodeExists for /brokers/seqid (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,044] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x7 zxid:0xda txntype:-1 reqpath:n/a Error Path:/isr_change_notification Error:KeeperErrorCode = NodeExists for /isr_change_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,055] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x8 zxid:0xdb txntype:-1 reqpath:n/a Error Path:/latest_producer_id_block Error:KeeperErrorCode = NodeExists for /latest_producer_id_block (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,067] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0x9 zxid:0xdc txntype:-1 reqpath:n/a Error Path:/log_dir_event_notification Error:KeeperErrorCode = NodeExists for /log_dir_event_notification (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,076] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0xa zxid:0xdd txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,088] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0xb zxid:0xde txntype:-1 reqpath:n/a Error Path:/config/clients Error:KeeperErrorCode = NodeExists for /config/clients (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,099] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0xc zxid:0xdf txntype:-1 reqpath:n/a Error Path:/config/users Error:KeeperErrorCode = NodeExists for /config/users (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,110] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:create cxid:0xd zxid:0xe0 txntype:-1 reqpath:n/a Error Path:/config/brokers Error:KeeperErrorCode = NodeExists for /config/brokers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:49,336] INFO Cluster ID = D76h3nR_S2mriu6Y-ZJNkg (kafka.server.KafkaServer)
[2018-08-14 12:34:49,402] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:34:49,414] INFO KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.1-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka-logs
	log.dirs = /tmp/kafka-logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.1-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	port = 9092
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 1
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 1
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = localhost:2181
	zookeeper.connection.timeout.ms = 6000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000
 (kafka.server.KafkaConfig)
[2018-08-14 12:34:49,440] INFO [ThrottledRequestReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:34:49,442] INFO [ThrottledRequestReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:34:49,443] INFO [ThrottledRequestReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:34:49,483] INFO Loading logs. (kafka.log.LogManager)
[2018-08-14 12:34:49,526] WARN [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-38/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-38/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,566] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,574] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,575] INFO [Log partition=__consumer_offsets-38, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 65 ms (kafka.log.Log)
[2018-08-14 12:34:49,587] WARN [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-36/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-36/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,589] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,591] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,592] INFO [Log partition=__consumer_offsets-36, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 6 ms (kafka.log.Log)
[2018-08-14 12:34:49,597] WARN [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-47/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-47/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,598] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,599] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,600] INFO [Log partition=__consumer_offsets-47, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,604] WARN [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-12/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-12/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,605] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,606] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,607] INFO [Log partition=__consumer_offsets-12, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,612] WARN [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-18/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-18/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,613] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,614] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,615] INFO [Log partition=__consumer_offsets-18, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,619] WARN [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-34/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-34/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,620] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,621] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,622] INFO [Log partition=__consumer_offsets-34, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,626] WARN [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-22/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-22/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,627] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,628] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,628] INFO [Log partition=__consumer_offsets-22, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,633] WARN [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-15/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-15/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,634] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,635] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,635] INFO [Log partition=__consumer_offsets-15, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,639] WARN [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-48/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-48/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,640] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,641] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,642] INFO [Log partition=__consumer_offsets-48, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,647] WARN [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-14/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-14/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,648] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,649] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,649] INFO [Log partition=__consumer_offsets-14, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,653] WARN [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-26/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-26/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,654] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,655] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,656] INFO [Log partition=__consumer_offsets-26, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,660] WARN [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-3/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-3/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,661] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,661] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,662] INFO [Log partition=__consumer_offsets-3, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,666] WARN [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-23/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-23/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,667] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,668] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,669] INFO [Log partition=__consumer_offsets-23, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,673] WARN [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-4/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-4/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,673] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,674] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,675] INFO [Log partition=__consumer_offsets-4, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,678] WARN [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-8/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-8/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,679] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,680] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,681] INFO [Log partition=__consumer_offsets-8, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,685] WARN [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-16/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-16/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,686] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,688] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,688] INFO [Log partition=__consumer_offsets-16, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,692] WARN [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-21/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-21/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,693] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,694] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,694] INFO [Log partition=__consumer_offsets-21, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,698] WARN [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-24/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-24/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,698] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,699] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,700] INFO [Log partition=__consumer_offsets-24, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,703] WARN [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-11/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-11/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,704] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,705] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,706] INFO [Log partition=__consumer_offsets-11, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,709] WARN [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-10/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-10/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,711] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,712] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,713] INFO [Log partition=__consumer_offsets-10, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2018-08-14 12:34:49,719] WARN [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/bitcoin_transaction_topic-0/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/bitcoin_transaction_topic-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,753] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Writing producer snapshot at offset 3539 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,756] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,782] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Writing producer snapshot at offset 3539 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,787] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 3539 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,791] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/bitcoin_transaction_topic-0/00000000000000003539.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,806] INFO [Log partition=bitcoin_transaction_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3539 in 88 ms (kafka.log.Log)
[2018-08-14 12:34:49,810] WARN [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-17/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-17/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,811] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,812] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,812] INFO [Log partition=__consumer_offsets-17, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,818] WARN [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-0/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,819] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,829] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,830] INFO [Log partition=__consumer_offsets-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 14 ms (kafka.log.Log)
[2018-08-14 12:34:49,835] WARN [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-45/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-45/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,836] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,837] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,838] INFO [Log partition=__consumer_offsets-45, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2018-08-14 12:34:49,841] WARN [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-32/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-32/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,842] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,843] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,843] INFO [Log partition=__consumer_offsets-32, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:34:49,847] WARN [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-2/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-2/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,847] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,848] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,849] INFO [Log partition=__consumer_offsets-2, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,852] WARN [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-28/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-28/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,852] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,853] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,854] INFO [Log partition=__consumer_offsets-28, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,857] WARN [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/twitter_topic-0/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/twitter_topic-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,861] INFO [ProducerStateManager partition=twitter_topic-0] Writing producer snapshot at offset 771 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,861] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,865] INFO [ProducerStateManager partition=twitter_topic-0] Writing producer snapshot at offset 771 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,866] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 771 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,866] INFO [ProducerStateManager partition=twitter_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/twitter_topic-0/00000000000000000771.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,867] INFO [Log partition=twitter_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 771 in 11 ms (kafka.log.Log)
[2018-08-14 12:34:49,872] WARN [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-39/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-39/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,873] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,875] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,875] INFO [Log partition=__consumer_offsets-39, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 5 ms (kafka.log.Log)
[2018-08-14 12:34:49,879] WARN [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-6/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-6/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,880] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,882] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,882] INFO [Log partition=__consumer_offsets-6, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,885] WARN [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-29/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-29/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,886] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,887] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,887] INFO [Log partition=__consumer_offsets-29, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:34:49,890] WARN [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-37/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-37/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,893] INFO [ProducerStateManager partition=__consumer_offsets-37] Writing producer snapshot at offset 73 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,894] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,896] INFO [ProducerStateManager partition=__consumer_offsets-37] Writing producer snapshot at offset 73 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,897] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Loading producer state from offset 73 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,897] INFO [ProducerStateManager partition=__consumer_offsets-37] Loading producer state from snapshot file '/tmp/kafka-logs/__consumer_offsets-37/00000000000000000073.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,898] INFO [Log partition=__consumer_offsets-37, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 73 in 9 ms (kafka.log.Log)
[2018-08-14 12:34:49,901] WARN [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-27/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-27/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,901] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,902] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,903] INFO [Log partition=__consumer_offsets-27, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,905] WARN [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-1/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-1/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,906] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,907] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,907] INFO [Log partition=__consumer_offsets-1, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:34:49,911] WARN [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-30/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-30/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,912] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,914] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,914] INFO [Log partition=__consumer_offsets-30, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,918] WARN [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-9/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-9/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,919] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,921] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,921] INFO [Log partition=__consumer_offsets-9, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,926] WARN [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__confluent.support.metrics-0/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__confluent.support.metrics-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,927] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,927] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,929] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Writing producer snapshot at offset 3 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,929] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Loading producer state from offset 3 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,930] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Loading producer state from snapshot file '/tmp/kafka-logs/__confluent.support.metrics-0/00000000000000000003.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,930] INFO [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 3 in 6 ms (kafka.log.Log)
[2018-08-14 12:34:49,933] WARN [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-40/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-40/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,934] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,934] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,935] INFO [Log partition=__consumer_offsets-40, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,938] WARN [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-13/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-13/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,939] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,939] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,940] INFO [Log partition=__consumer_offsets-13, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,942] WARN [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-19/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-19/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,943] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,944] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,944] INFO [Log partition=__consumer_offsets-19, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 2 ms (kafka.log.Log)
[2018-08-14 12:34:49,947] WARN [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/bitcoin_price_topic-0/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/bitcoin_price_topic-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,948] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Writing producer snapshot at offset 485 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,949] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,950] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Writing producer snapshot at offset 485 (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,951] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Loading producer state from offset 485 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,951] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Loading producer state from snapshot file '/tmp/kafka-logs/bitcoin_price_topic-0/00000000000000000485.snapshot' (kafka.log.ProducerStateManager)
[2018-08-14 12:34:49,952] INFO [Log partition=bitcoin_price_topic-0, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 485 in 6 ms (kafka.log.Log)
[2018-08-14 12:34:49,954] WARN [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-46/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-46/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,955] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,956] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,957] INFO [Log partition=__consumer_offsets-46, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,960] WARN [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-7/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-7/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,961] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,962] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,963] INFO [Log partition=__consumer_offsets-7, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,967] WARN [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-43/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-43/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,968] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,969] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,970] INFO [Log partition=__consumer_offsets-43, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,973] WARN [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-35/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-35/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,974] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,975] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,975] INFO [Log partition=__consumer_offsets-35, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,979] WARN [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-42/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-42/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,979] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,980] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,981] INFO [Log partition=__consumer_offsets-42, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:49,985] WARN [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-20/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-20/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,986] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,987] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,988] INFO [Log partition=__consumer_offsets-20, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,992] WARN [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-49/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-49/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:49,993] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:49,994] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:49,995] INFO [Log partition=__consumer_offsets-49, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:49,999] WARN [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-44/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-44/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,000] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,001] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,001] INFO [Log partition=__consumer_offsets-44, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:50,004] WARN [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-41/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-41/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,005] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,006] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,006] INFO [Log partition=__consumer_offsets-41, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:50,009] WARN [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-31/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-31/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,010] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,011] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,011] INFO [Log partition=__consumer_offsets-31, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:50,015] WARN [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-33/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-33/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,016] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,018] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,018] INFO [Log partition=__consumer_offsets-33, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:50,022] WARN [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-25/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-25/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,023] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,024] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,024] INFO [Log partition=__consumer_offsets-25, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 3 ms (kafka.log.Log)
[2018-08-14 12:34:50,027] WARN [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Found a corrupted index file corresponding to log file /tmp/kafka-logs/__consumer_offsets-5/00000000000000000000.log due to Corrupt index found, index file (/tmp/kafka-logs/__consumer_offsets-5/00000000000000000000.index) has non-zero size but the last offset is 0 which is no greater than the base offset 0.}, recovering segment and rebuilding index files... (kafka.log.Log)
[2018-08-14 12:34:50,028] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Recovering unflushed segment 0 (kafka.log.Log)
[2018-08-14 12:34:50,030] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Loading producer state from offset 0 with message format version 2 (kafka.log.Log)
[2018-08-14 12:34:50,030] INFO [Log partition=__consumer_offsets-5, dir=/tmp/kafka-logs] Completed load of log with 1 segments, log start offset 0 and log end offset 0 in 4 ms (kafka.log.Log)
[2018-08-14 12:34:50,033] INFO Logs loading complete in 550 ms. (kafka.log.LogManager)
[2018-08-14 12:34:50,041] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
[2018-08-14 12:34:50,042] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
[2018-08-14 12:34:50,273] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.Acceptor)
[2018-08-14 12:34:50,308] INFO [SocketServer brokerId=0] Started 1 acceptor threads (kafka.network.SocketServer)
[2018-08-14 12:34:50,325] INFO [ExpirationReaper-0-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,327] INFO [ExpirationReaper-0-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,332] INFO [ExpirationReaper-0-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,344] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:34:50,409] INFO Creating /brokers/ids/0 (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:34:50,427] INFO Result of znode creation at /brokers/ids/0 is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:34:50,428] INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(narendra-LT,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)
[2018-08-14 12:34:50,483] INFO Creating /controller (is it secure? false) (kafka.zk.KafkaZkClient)
[2018-08-14 12:34:50,484] INFO [ExpirationReaper-0-topic]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,488] INFO [ExpirationReaper-0-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,489] INFO [ExpirationReaper-0-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:34:50,496] INFO Result of znode creation at /controller is: OK (kafka.zk.KafkaZkClient)
[2018-08-14 12:34:50,515] INFO [GroupCoordinator 0]: Starting up. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:34:50,516] INFO [GroupCoordinator 0]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:34:50,518] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 2 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:50,538] INFO [ProducerId Manager 0]: Acquired new producerId block (brokerId:0,blockStartProducerId:2000,blockEndProducerId:2999) by writing to Zk with path version 3 (kafka.coordinator.transaction.ProducerIdManager)
[2018-08-14 12:34:50,570] INFO [TransactionCoordinator id=0] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:34:50,571] INFO [Transaction Marker Channel Manager 0]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:34:50,572] INFO [TransactionCoordinator id=0] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:34:50,612] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:34:50,626] WARN [Log partition=__confluent.support.metrics-0, dir=/tmp/kafka-logs] retention.ms for topic __confluent.support.metrics is set to 31536000000. It is smaller than message.timestamp.difference.max.ms's value 9223372036854775807. This may result in frequent log rolling. (kafka.log.Log)
[2018-08-14 12:34:50,645] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:34:50,646] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:34:50,647] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2018-08-14 12:34:50,651] INFO Waiting 10009 ms for the monitored service to finish starting up... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:34:50,817] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0000 type:delete cxid:0x72 zxid:0xe5 txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:34:50,854] INFO [ReplicaFetcherManager on broker 0] Removed fetcher for partitions __consumer_offsets-22,__consumer_offsets-30,__consumer_offsets-8,__consumer_offsets-21,__consumer_offsets-4,__consumer_offsets-27,__consumer_offsets-7,__consumer_offsets-9,__consumer_offsets-46,__consumer_offsets-25,__consumer_offsets-35,__consumer_offsets-41,__consumer_offsets-33,__consumer_offsets-23,__consumer_offsets-49,__consumer_offsets-47,__consumer_offsets-16,__consumer_offsets-28,__consumer_offsets-31,__consumer_offsets-36,__consumer_offsets-42,__consumer_offsets-3,__consumer_offsets-18,__consumer_offsets-37,__consumer_offsets-15,__consumer_offsets-24,bitcoin_price_topic-0,bitcoin_transaction_topic-0,__consumer_offsets-38,__consumer_offsets-17,twitter_topic-0,__consumer_offsets-48,__confluent.support.metrics-0,__consumer_offsets-19,__consumer_offsets-11,__consumer_offsets-13,__consumer_offsets-2,__consumer_offsets-43,__consumer_offsets-6,__consumer_offsets-14,__consumer_offsets-20,__consumer_offsets-0,__consumer_offsets-44,__consumer_offsets-39,__consumer_offsets-12,__consumer_offsets-45,__consumer_offsets-1,__consumer_offsets-5,__consumer_offsets-26,__consumer_offsets-29,__consumer_offsets-34,__consumer_offsets-10,__consumer_offsets-32,__consumer_offsets-40 (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:34:50,869] INFO Replica loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,871] INFO [Partition __consumer_offsets-0 broker=0] __consumer_offsets-0 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,884] INFO Replica loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,884] INFO [Partition __consumer_offsets-29 broker=0] __consumer_offsets-29 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,890] INFO Replica loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,890] INFO [Partition __consumer_offsets-48 broker=0] __consumer_offsets-48 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,896] INFO Replica loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,896] INFO [Partition __consumer_offsets-10 broker=0] __consumer_offsets-10 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,900] INFO Replica loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,901] INFO [Partition __consumer_offsets-45 broker=0] __consumer_offsets-45 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,905] INFO Replica loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,905] INFO [Partition __consumer_offsets-26 broker=0] __consumer_offsets-26 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,910] INFO Replica loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,910] INFO [Partition __consumer_offsets-7 broker=0] __consumer_offsets-7 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,915] INFO Replica loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,915] INFO [Partition __consumer_offsets-42 broker=0] __consumer_offsets-42 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,919] INFO Replica loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,920] INFO [Partition __consumer_offsets-4 broker=0] __consumer_offsets-4 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,928] INFO Replica loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,928] INFO [Partition __consumer_offsets-23 broker=0] __consumer_offsets-23 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,932] INFO Replica loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,932] INFO [Partition __consumer_offsets-1 broker=0] __consumer_offsets-1 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,936] INFO Replica loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,936] INFO [Partition __consumer_offsets-39 broker=0] __consumer_offsets-39 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,940] INFO Replica loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,940] INFO [Partition __consumer_offsets-20 broker=0] __consumer_offsets-20 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,944] INFO Replica loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,945] INFO [Partition __consumer_offsets-17 broker=0] __consumer_offsets-17 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,948] INFO Replica loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,949] INFO [Partition __consumer_offsets-36 broker=0] __consumer_offsets-36 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,953] INFO Replica loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,953] INFO [Partition __consumer_offsets-14 broker=0] __consumer_offsets-14 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,956] INFO Replica loaded for partition bitcoin_price_topic-0 with initial high watermark 485 (kafka.cluster.Replica)
[2018-08-14 12:34:50,957] INFO [Partition bitcoin_price_topic-0 broker=0] bitcoin_price_topic-0 starts at Leader Epoch 0 from offset 485. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,961] INFO Replica loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,961] INFO [Partition __consumer_offsets-33 broker=0] __consumer_offsets-33 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,964] INFO Replica loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,965] INFO [Partition __consumer_offsets-49 broker=0] __consumer_offsets-49 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,968] INFO Replica loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,968] INFO [Partition __consumer_offsets-11 broker=0] __consumer_offsets-11 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,972] INFO Replica loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,973] INFO [Partition __consumer_offsets-30 broker=0] __consumer_offsets-30 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,977] INFO Replica loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,977] INFO [Partition __consumer_offsets-46 broker=0] __consumer_offsets-46 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,982] INFO Replica loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,982] INFO [Partition __consumer_offsets-27 broker=0] __consumer_offsets-27 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,986] INFO Replica loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,986] INFO [Partition __consumer_offsets-8 broker=0] __consumer_offsets-8 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,989] INFO Replica loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,990] INFO [Partition __consumer_offsets-24 broker=0] __consumer_offsets-24 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,993] INFO Replica loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,993] INFO [Partition __consumer_offsets-43 broker=0] __consumer_offsets-43 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:50,997] INFO Replica loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:50,997] INFO [Partition __consumer_offsets-5 broker=0] __consumer_offsets-5 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,001] INFO Replica loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,001] INFO [Partition __consumer_offsets-21 broker=0] __consumer_offsets-21 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,005] INFO Replica loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,005] INFO [Partition __consumer_offsets-2 broker=0] __consumer_offsets-2 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,008] INFO Replica loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,008] INFO [Partition __consumer_offsets-40 broker=0] __consumer_offsets-40 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,011] INFO Replica loaded for partition __consumer_offsets-37 with initial high watermark 73 (kafka.cluster.Replica)
[2018-08-14 12:34:51,011] INFO [Partition __consumer_offsets-37 broker=0] __consumer_offsets-37 starts at Leader Epoch 0 from offset 73. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,015] INFO Replica loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,015] INFO [Partition __consumer_offsets-18 broker=0] __consumer_offsets-18 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,018] INFO Replica loaded for partition __confluent.support.metrics-0 with initial high watermark 3 (kafka.cluster.Replica)
[2018-08-14 12:34:51,019] INFO [Partition __confluent.support.metrics-0 broker=0] __confluent.support.metrics-0 starts at Leader Epoch 0 from offset 3. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,022] INFO Replica loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,022] INFO [Partition __consumer_offsets-34 broker=0] __consumer_offsets-34 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,025] INFO Replica loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,025] INFO [Partition __consumer_offsets-15 broker=0] __consumer_offsets-15 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,029] INFO Replica loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,029] INFO [Partition __consumer_offsets-12 broker=0] __consumer_offsets-12 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,031] INFO Replica loaded for partition twitter_topic-0 with initial high watermark 771 (kafka.cluster.Replica)
[2018-08-14 12:34:51,031] INFO [Partition twitter_topic-0 broker=0] twitter_topic-0 starts at Leader Epoch 0 from offset 771. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,034] INFO Replica loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,035] INFO [Partition __consumer_offsets-31 broker=0] __consumer_offsets-31 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,038] INFO Replica loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,038] INFO [Partition __consumer_offsets-9 broker=0] __consumer_offsets-9 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,041] INFO Replica loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,042] INFO [Partition __consumer_offsets-47 broker=0] __consumer_offsets-47 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,045] INFO Replica loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,045] INFO [Partition __consumer_offsets-19 broker=0] __consumer_offsets-19 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,048] INFO Replica loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,049] INFO [Partition __consumer_offsets-28 broker=0] __consumer_offsets-28 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,053] INFO Replica loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,054] INFO [Partition __consumer_offsets-38 broker=0] __consumer_offsets-38 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,057] INFO Replica loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,057] INFO [Partition __consumer_offsets-35 broker=0] __consumer_offsets-35 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,060] INFO Replica loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,061] INFO [Partition __consumer_offsets-6 broker=0] __consumer_offsets-6 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,064] INFO Replica loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,064] INFO [Partition __consumer_offsets-44 broker=0] __consumer_offsets-44 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,068] INFO Replica loaded for partition bitcoin_transaction_topic-0 with initial high watermark 3539 (kafka.cluster.Replica)
[2018-08-14 12:34:51,068] INFO [Partition bitcoin_transaction_topic-0 broker=0] bitcoin_transaction_topic-0 starts at Leader Epoch 0 from offset 3539. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,071] INFO Replica loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,071] INFO [Partition __consumer_offsets-25 broker=0] __consumer_offsets-25 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,074] INFO Replica loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,074] INFO [Partition __consumer_offsets-16 broker=0] __consumer_offsets-16 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,077] INFO Replica loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,077] INFO [Partition __consumer_offsets-22 broker=0] __consumer_offsets-22 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,080] INFO Replica loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,080] INFO [Partition __consumer_offsets-41 broker=0] __consumer_offsets-41 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,083] INFO Replica loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,083] INFO [Partition __consumer_offsets-32 broker=0] __consumer_offsets-32 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,087] INFO Replica loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,087] INFO [Partition __consumer_offsets-3 broker=0] __consumer_offsets-3 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,090] INFO Replica loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Replica)
[2018-08-14 12:34:51,090] INFO [Partition __consumer_offsets-13 broker=0] __consumer_offsets-13 starts at Leader Epoch 0 from offset 0. Previous Leader Epoch was: -1 (kafka.cluster.Partition)
[2018-08-14 12:34:51,103] INFO [ReplicaAlterLogDirsManager on broker 0] Added fetcher for partitions List() (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:34:51,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,105] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,106] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,107] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,108] INFO [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,128] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-22 in 22 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-28 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,129] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-31 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,130] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-34 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,165] INFO [GroupCoordinator 0]: Loading group metadata for flume with generation 4 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:34:51,166] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-37 in 36 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,166] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-40 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,166] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-43 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,167] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-46 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,167] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-49 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,167] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-25 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,167] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,168] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,168] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,168] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,169] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,169] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-7 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,169] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-10 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,169] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-13 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-16 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-19 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,170] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,171] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,172] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-6 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,173] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-9 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-12 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-18 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-21 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-24 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,174] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-27 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,175] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-30 in 1 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,175] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-33 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,175] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-36 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,175] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-39 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,175] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-42 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,176] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-45 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,176] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-48 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:34:51,176] INFO [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-15 in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:35:00,663] INFO Monitored service is now ready (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:35:00,664] INFO Starting metrics collection from monitored component... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:35:00,953] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:35:00,972] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://narendra-LT:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2018-08-14 12:35:01,005] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:35:01,005] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:35:01,045] INFO Cluster ID: D76h3nR_S2mriu6Y-ZJNkg (org.apache.kafka.clients.Metadata)
[2018-08-14 12:35:01,097] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2018-08-14 12:35:01,104] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2018-08-14 12:35:02,996] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2018-08-14 12:36:00,870] INFO Accepted socket connection from /127.0.0.1:33300 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:36:00,877] INFO Client attempting to establish new session at /127.0.0.1:33300 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:00,887] INFO Established session 0x1653740312d0001 with negotiated timeout 30000 for client /127.0.0.1:33300 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:01,554] INFO Processed session termination for sessionid: 0x1653740312d0001 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:36:01,587] INFO Closed socket connection for client /127.0.0.1:33300 which had sessionid 0x1653740312d0001 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:36:07,915] INFO Accepted socket connection from /127.0.0.1:33318 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:36:07,919] INFO Client attempting to establish new session at /127.0.0.1:33318 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:07,939] INFO Established session 0x1653740312d0002 with negotiated timeout 30000 for client /127.0.0.1:33318 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:08,231] INFO Processed session termination for sessionid: 0x1653740312d0002 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:36:08,260] INFO Closed socket connection for client /127.0.0.1:33318 which had sessionid 0x1653740312d0002 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:36:14,663] INFO Accepted socket connection from /127.0.0.1:33324 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:36:14,667] INFO Client attempting to establish new session at /127.0.0.1:33324 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:14,715] INFO Established session 0x1653740312d0003 with negotiated timeout 30000 for client /127.0.0.1:33324 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:36:15,029] INFO Processed session termination for sessionid: 0x1653740312d0003 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:36:15,080] INFO Closed socket connection for client /127.0.0.1:33324 which had sessionid 0x1653740312d0003 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:37:10,484] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 4 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:37:10,576] INFO [GroupCoordinator 0]: Stabilized group flume generation 5 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:37:10,638] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 5 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:40:13,768] INFO Accepted socket connection from /127.0.0.1:37952 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:40:13,792] INFO Client attempting to establish new session at /127.0.0.1:37952 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:13,833] INFO Established session 0x1653740312d0004 with negotiated timeout 30000 for client /127.0.0.1:37952 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:13,917] INFO Processed session termination for sessionid: 0x1653740312d0004 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:40:13,927] INFO Closed socket connection for client /127.0.0.1:37952 which had sessionid 0x1653740312d0004 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:40:47,924] INFO Accepted socket connection from /127.0.0.1:38734 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:40:47,927] INFO Client attempting to establish new session at /127.0.0.1:38734 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:47,945] INFO Established session 0x1653740312d0005 with negotiated timeout 30000 for client /127.0.0.1:38734 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:48,205] INFO Accepted socket connection from /127.0.0.1:38754 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:40:48,206] INFO Client attempting to establish new session at /127.0.0.1:38754 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:48,222] INFO Established session 0x1653740312d0006 with negotiated timeout 6000 for client /127.0.0.1:38754 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:40:49,021] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0006 type:create cxid:0x2 zxid:0xf0 txntype:-1 reqpath:n/a Error Path:/consumers Error:KeeperErrorCode = NodeExists for /consumers (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:40:49,612] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0006 type:create cxid:0x19 zxid:0xf4 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-85678/owners/bitcoin_price_topic Error:KeeperErrorCode = NoNode for /consumers/console-consumer-85678/owners/bitcoin_price_topic (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:40:49,740] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0006 type:create cxid:0x1a zxid:0xf5 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-85678/owners Error:KeeperErrorCode = NoNode for /consumers/console-consumer-85678/owners (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:41:48,284] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0006 type:setData cxid:0x24 zxid:0xf9 txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-85678/offsets/bitcoin_price_topic/0 Error:KeeperErrorCode = NoNode for /consumers/console-consumer-85678/offsets/bitcoin_price_topic/0 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:41:48,333] INFO Got user-level KeeperException when processing sessionid:0x1653740312d0006 type:create cxid:0x25 zxid:0xfa txntype:-1 reqpath:n/a Error Path:/consumers/console-consumer-85678/offsets Error:KeeperErrorCode = NoNode for /consumers/console-consumer-85678/offsets (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:42:12,100] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 5 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:42:14,760] INFO [GroupCoordinator 0]: Stabilized group flume generation 6 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:42:14,762] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 6 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:42:51,377] INFO Processed session termination for sessionid: 0x1653740312d0006 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:42:51,379] INFO Accepted socket connection from /127.0.0.1:41674 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-08-14 12:42:51,379] INFO Client attempting to establish new session at /127.0.0.1:41674 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:42:51,400] INFO Closed socket connection for client /127.0.0.1:38754 which had sessionid 0x1653740312d0006 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:42:51,432] INFO Established session 0x1653740312d0007 with negotiated timeout 30000 for client /127.0.0.1:41674 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:42:51,565] INFO Processed session termination for sessionid: 0x1653740312d0007 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:42:51,576] INFO Closed socket connection for client /127.0.0.1:41674 which had sessionid 0x1653740312d0007 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:42:51,907] WARN caught end of stream exception (org.apache.zookeeper.server.NIOServerCnxn)
EndOfStreamException: Unable to read additional data from client sessionid 0x1653740312d0005, likely client has closed socket
	at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)
	at java.lang.Thread.run(Thread.java:748)
[2018-08-14 12:42:51,931] INFO Closed socket connection for client /127.0.0.1:38734 which had sessionid 0x1653740312d0005 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:43:21,001] INFO Expiring session 0x1653740312d0005, timeout of 30000ms exceeded (org.apache.zookeeper.server.ZooKeeperServer)
[2018-08-14 12:43:21,002] INFO Processed session termination for sessionid: 0x1653740312d0005 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:44:50,534] INFO [GroupMetadataManager brokerId=0] Removed 0 expired offsets in 18 milliseconds. (kafka.coordinator.group.GroupMetadataManager)
[2018-08-14 12:46:13,868] INFO [GroupCoordinator 0]: Preparing to rebalance group flume with old generation 6 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:46:17,902] INFO [GroupCoordinator 0]: Stabilized group flume generation 7 (__consumer_offsets-37) (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:46:17,903] INFO [GroupCoordinator 0]: Assignment received from leader for group flume for generation 7 (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:49:03,936] INFO Terminating process due to signal SIGTERM (io.confluent.support.metrics.SupportedKafka)
[2018-08-14 12:49:03,965] WARN The replication factor of topic __confluent.support.metrics is 1, which is less than the desired replication factor of 3.  If you happen to add more brokers to this cluster, then it is important to increase the replication factor of the topic to eventually 3 to ensure reliable and durable metrics collection. (io.confluent.support.metrics.common.kafka.KafkaUtilities)
[2018-08-14 12:49:03,971] INFO ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [PLAINTEXT://narendra-LT:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	confluent.batch.expiry.ms = 30000
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 10000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig)
[2018-08-14 12:49:03,979] INFO Kafka version : 1.1.0-cp1 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:49:03,980] INFO Kafka commitId : 93e03414f72c2485 (org.apache.kafka.common.utils.AppInfoParser)
[2018-08-14 12:49:03,998] INFO Cluster ID: D76h3nR_S2mriu6Y-ZJNkg (org.apache.kafka.clients.Metadata)
[2018-08-14 12:49:04,022] INFO [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
[2018-08-14 12:49:04,025] INFO Successfully submitted metrics to Kafka topic __confluent.support.metrics (io.confluent.support.metrics.submitters.KafkaSubmitter)
[2018-08-14 12:49:05,218] INFO Successfully submitted metrics to Confluent via secure endpoint (io.confluent.support.metrics.submitters.ConfluentSubmitter)
[2018-08-14 12:49:05,218] INFO Graceful terminating metrics collection because the monitored component is shutting down... (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:49:05,218] INFO Metrics collection stopped (io.confluent.support.metrics.BaseMetricsReporter)
[2018-08-14 12:49:05,229] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2018-08-14 12:49:05,234] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2018-08-14 12:49:05,322] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2018-08-14 12:49:05,348] INFO [/config/changes-event-process-thread]: Shutting down (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:49:05,358] INFO [/config/changes-event-process-thread]: Stopped (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:49:05,359] INFO [/config/changes-event-process-thread]: Shutdown completed (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2018-08-14 12:49:05,362] INFO [SocketServer brokerId=0] Stopping socket server request processors (kafka.network.SocketServer)
[2018-08-14 12:49:05,375] INFO [SocketServer brokerId=0] Stopped socket server request processors (kafka.network.SocketServer)
[2018-08-14 12:49:05,376] INFO [Kafka Request Handler on Broker 0], shutting down (kafka.server.KafkaRequestHandlerPool)
[2018-08-14 12:49:05,390] INFO [Kafka Request Handler on Broker 0], shut down completely (kafka.server.KafkaRequestHandlerPool)
[2018-08-14 12:49:05,401] INFO [KafkaApi-0] Shutdown complete. (kafka.server.KafkaApis)
[2018-08-14 12:49:05,402] INFO [ExpirationReaper-0-topic]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,532] INFO [ExpirationReaper-0-topic]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,532] INFO [ExpirationReaper-0-topic]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,543] INFO [TransactionCoordinator id=0] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:49:05,545] INFO [ProducerId Manager 0]: Shutdown complete: last producerId assigned 2000 (kafka.coordinator.transaction.ProducerIdManager)
[2018-08-14 12:49:05,545] INFO [Transaction State Manager 0]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
[2018-08-14 12:49:05,545] INFO [Transaction Marker Channel Manager 0]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:49:05,546] INFO [Transaction Marker Channel Manager 0]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:49:05,546] INFO [Transaction Marker Channel Manager 0]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
[2018-08-14 12:49:05,546] INFO [TransactionCoordinator id=0] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
[2018-08-14 12:49:05,549] INFO [GroupCoordinator 0]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:49:05,549] INFO [ExpirationReaper-0-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,682] INFO [ExpirationReaper-0-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,682] INFO [ExpirationReaper-0-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,682] INFO [ExpirationReaper-0-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,746] INFO [ExpirationReaper-0-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,746] INFO [ExpirationReaper-0-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,747] INFO [GroupCoordinator 0]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
[2018-08-14 12:49:05,748] INFO [ReplicaManager broker=0] Shutting down (kafka.server.ReplicaManager)
[2018-08-14 12:49:05,748] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:49:05,748] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:49:05,749] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
[2018-08-14 12:49:05,749] INFO [ReplicaFetcherManager on broker 0] shutting down (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:49:05,751] INFO [ReplicaFetcherManager on broker 0] shutdown completed (kafka.server.ReplicaFetcherManager)
[2018-08-14 12:49:05,751] INFO [ReplicaAlterLogDirsManager on broker 0] shutting down (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:49:05,751] INFO [ReplicaAlterLogDirsManager on broker 0] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
[2018-08-14 12:49:05,751] INFO [ExpirationReaper-0-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,828] INFO [ExpirationReaper-0-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,829] INFO [ExpirationReaper-0-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,829] INFO [ExpirationReaper-0-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,909] INFO [ExpirationReaper-0-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,909] INFO [ExpirationReaper-0-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,909] INFO [ExpirationReaper-0-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,980] INFO [ExpirationReaper-0-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:05,980] INFO [ExpirationReaper-0-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
[2018-08-14 12:49:06,117] INFO [ReplicaManager broker=0] Shut down completely (kafka.server.ReplicaManager)
[2018-08-14 12:49:06,122] INFO Shutting down. (kafka.log.LogManager)
[2018-08-14 12:49:06,219] INFO [ProducerStateManager partition=__consumer_offsets-37] Writing producer snapshot at offset 182 (kafka.log.ProducerStateManager)
[2018-08-14 12:49:06,221] INFO [ProducerStateManager partition=bitcoin_price_topic-0] Writing producer snapshot at offset 969 (kafka.log.ProducerStateManager)
[2018-08-14 12:49:06,221] INFO [ProducerStateManager partition=bitcoin_transaction_topic-0] Writing producer snapshot at offset 6532 (kafka.log.ProducerStateManager)
[2018-08-14 12:49:06,222] INFO [ProducerStateManager partition=twitter_topic-0] Writing producer snapshot at offset 1958 (kafka.log.ProducerStateManager)
[2018-08-14 12:49:06,235] INFO [ProducerStateManager partition=__confluent.support.metrics-0] Writing producer snapshot at offset 5 (kafka.log.ProducerStateManager)
[2018-08-14 12:49:06,703] INFO Shutdown complete. (kafka.log.LogManager)
[2018-08-14 12:49:06,995] INFO [ZooKeeperClient] Closing. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:49:06,996] INFO Processed session termination for sessionid: 0x1653740312d0000 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-08-14 12:49:07,062] INFO Closed socket connection for client /127.0.0.1:33216 which had sessionid 0x1653740312d0000 (org.apache.zookeeper.server.NIOServerCnxn)
[2018-08-14 12:49:07,062] INFO Session: 0x1653740312d0000 closed (org.apache.zookeeper.ZooKeeper)
[2018-08-14 12:49:07,121] INFO EventThread shut down for session: 0x1653740312d0000 (org.apache.zookeeper.ClientCnxn)
[2018-08-14 12:49:07,121] INFO [ZooKeeperClient] Closed. (kafka.zookeeper.ZooKeeperClient)
[2018-08-14 12:49:07,134] INFO [ThrottledRequestReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:07,669] INFO [ThrottledRequestReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:07,669] INFO [ThrottledRequestReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:07,669] INFO [ThrottledRequestReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:08,669] INFO [ThrottledRequestReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:08,669] INFO [ThrottledRequestReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:08,670] INFO [ThrottledRequestReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:09,662] INFO [ThrottledRequestReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:09,662] INFO [ThrottledRequestReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledRequestReaper)
[2018-08-14 12:49:09,685] INFO [SocketServer brokerId=0] Shutting down socket server (kafka.network.SocketServer)
[2018-08-14 12:49:09,730] INFO [SocketServer brokerId=0] Shutdown completed (kafka.network.SocketServer)
[2018-08-14 12:49:09,737] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
